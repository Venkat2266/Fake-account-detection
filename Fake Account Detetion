# Import Libraries
import numpy as num
import pandas as pd
import matplotlib.pyplot as graph
import seaborn as sb
import sys
import warnings
if not sys.warnoptions:
    warnings.simplefilter("ignore")

#To check Performances
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score

def evaluateModel(input_data , algo_output):
    Modelacc = accuracy_score(input_data , algo_output) * 100
    modelpreci = precision_score(input_data , algo_output) * 100
    ModelRecal = recall_score(input_data , algo_output) * 100
    finalscore = f1_score(input_data , algo_output, average='weighted')
    print('Modelacc is {:.4f}%\n ModelPrecision is {:.4f}%\n ModelRecall is {:.4f}%\nModelF1 Score is {:.4f}\n'.format(Modelacc, modelpreci, ModelRecal, finalscore))

# Set Data
Features_labels = pd.read_csv("Botdataset.csv")
Label_B = Features_labels[Features_labels.bot==1]
Lable_NB = Features_labels[Features_labels.bot==0]

Features_labels.columns

Features_labels.head(10)

Features_labels.describe()

DatasetFeatureNames = ['id', 'id_str', 'screen_name', 'location', 'description', 'url',
       'followers_count', 'friends_count', 'listed_count', 'created_at',
       'favourites_count', 'verified', 'statuses_count', 'lang', 'status',
       'default_profile', 'default_profile_image', 'has_extended_profile',
       'name', 'bot']

maskingdata = num.zeros_like(Features_labels[DatasetFeatureNames].corr(), dtype = num.bool) 
maskingdata[num.triu_indices_from(maskingdata)] = True 

f, ax = graph.subplots(figsize=(16, 12))
graph.title('Pearson Correlation Matrix',fontsize=25)

sb.heatmap(Features_labels[DatasetFeatureNames].corr(),linewidths = 0.25,vmax = 0.7,square = True,cmap = "BuGn", 
            linecolor = 'w',annot = True,annot_kws = {"size":8},mask = maskingdata,cbar_kws = {"shrink": 0.9})

Features_labels.drop(['id_str', 'screen_name', 'location', 'description', 'url', 'created_at', 'lang', 'status', 'has_extended_profile','name'],axis=1,inplace=True)

Features_labels.head()

sb.catplot(x="bot", y="followers_count", data=Features_labels);
sb.catplot(x="bot", y="friends_count", data=Features_labels);
sb.catplot(x="bot", y="listed_count", data=Features_labels);

sb.catplot(x="bot", y="favourites_count", data=Features_labels);
sb.catplot(x="bot", y="verified", data=Features_labels);
sb.catplot(x="bot", y="statuses_count", data=Features_labels);

featuredata_X = Features_labels.iloc[:, :-1].values
Labeldata_y= Features_labels.iloc[:, 9].values

from sklearn.preprocessing import LabelEncoder
trainx=LabelEncoder()
featuredata_X[:,5]=trainx.fit_transform(featuredata_X[:,5])
featuredata_X[:,7]=trainx.fit_transform(featuredata_X[:,7])
featuredata_X[:,8]=trainx.fit_transform(featuredata_X[:,8])

# KNN

from sklearn.neighbors import KNeighborsClassifier as knn
knnclassifier=knn(n_neighbors=5)
knnclassifier.fit(featuredata_X,Labeldata_y)
Label_B = Features_labels[Features_labels.bot==1]
Lable_NB = Features_labels[Features_labels.bot==0]
 
knn_feature = Label_B.iloc[:,:-1]
knnlablel_y = Label_B.iloc[:,9]
knn_prediction = knnclassifier.predict(knn_feature)

#Confusionmatrix
from sklearn.metrics import confusion_matrix
cm=confusion_matrix(knnlablel_y,knn_prediction)
evaluateModel(knnlablel_y,knn_prediction)

feature_NB = Lable_NB.iloc[:,:-1]
label_NB_y = Lable_NB.iloc[:,9]
KNn_NB_pred = knnclassifier.predict(feature_NB)

#Confusionmatrix
from sklearn.metrics import confusion_matrix
cm=confusion_matrix(label_NB_y,KNn_NB_pred)
evaluateModel(label_NB_y,KNn_NB_pred)



from sklearn.naive_bayes import GaussianNB as GNB
classifier=GNB()
classifier.fit(featuredata_X,Labeldata_y)
Label_B = Features_labels[Features_labels.bot==1]
Lable_NB = Features_labels[Features_labels.bot==0]
 
GNNBFeature = Label_B.iloc[:,:-1]
GNNBLabel = Label_B.iloc[:,9]
GNNB_pred = classifier.predict(GNNBFeature)

#Confusionmatrix
from sklearn.metrics import confusion_matrix
cm=confusion_matrix(GNNBLabel,GNNB_pred)
evaluateModel(GNNBLabel,GNNB_pred)

GNNB_NB = Lable_NB.iloc[:,:-1]
GNNB_NB_y = Lable_NB.iloc[:,9]
GNNB_NB_pred = classifier.predict(GNNB_NB)

#Confusionmatrix
from sklearn.metrics import confusion_matrix
cm=confusion_matrix(GNNB_NB_y,GNNB_NB_pred)
evaluateModel(GNNB_NB_y,GNNB_NB_pred)

#fitting
from sklearn.ensemble import RandomForestClassifier as rf
classifier= rf(n_estimators=10,criterion='entropy',random_state=0)
classifier.fit(featuredata_X,Labeldata_y)
Label_B = Features_labels[Features_labels.bot==1]
Lable_NB = Features_labels[Features_labels.bot==0]

RFC_Feature = Label_B.iloc[:,:-1]
RFC_Label = Label_B.iloc[:,9]
RFC_pred = classifier.predict(RFC_Feature)

#Confusionmatrix
from sklearn.metrics import confusion_matrix
cm=confusion_matrix(RFC_Label,RFC_pred)
evaluateModel(RFC_Label,RFC_pred)

RFC_Feature_NB = Lable_NB.iloc[:,:-1]
RFC_Label_NB_y = Lable_NB.iloc[:,9]
RFC_NB_pred = classifier.predict(RFC_Feature_NB)
 
#Confusionmatrix
from sklearn.metrics import confusion_matrix
cm=confusion_matrix(RFC_Label_NB_y,RFC_NB_pred)
evaluateModel(RFC_Label_NB_y,RFC_NB_pred)

from sklearn.tree import DecisionTreeClassifier as DTC
classifier= DTC(criterion="entropy")
classifier.fit(featuredata_X,Labeldata_y)
Label_B = Features_labels[Features_labels.bot==1]
Lable_NB = Features_labels[Features_labels.bot==0]
 
DT_Feature_B = Label_B.iloc[:,:-1]
DT_Label_B = Label_B.iloc[:,9]
DT_B_pred = classifier.predict(DT_Feature_B)

#Confusionmatrix
from sklearn.metrics import confusion_matrix
cm=confusion_matrix(DT_Label_B,DT_B_pred)
evaluateModel(DT_Label_B,DT_B_pred)

DT_Feature_NB = Lable_NB.iloc[:,:-1]
DT_Label_NB_y = Lable_NB.iloc[:,9]
DT_NB_pred = classifier.predict(DT_Feature_NB)

#Confusionmatrix
from sklearn.metrics import confusion_matrix
cm=confusion_matrix(DT_Label_NB_y,DT_NB_pred)
evaluateModel(DT_Label_NB_y,DT_NB_pred)


from sklearn.datasets import make_classification
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from matplotlib import pyplot
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import f1_score
from sklearn.metrics import auc

Feature_X, Labels_y = make_classification(n_samples=1000, n_classes=2, random_state=1)
LR_train_Features, LR_test_Features, LR_train_Labels, LR_test_Labels = train_test_split(Feature_X, Labels_y, test_size=0.3, random_state=2)
LR_Prob = [0 for _ in range(len(LR_test_Labels))]
LRmodel = LogisticRegression(solver='lbfgs')
LRmodel.fit(LR_train_Features, LR_train_Labels)
logistic_probs = LRmodel.predict_proba(LR_test_Features)
logistic_probs = logistic_probs[:, 1]
lR_Prb_auc = roc_auc_score(LR_test_Labels, LR_Prob)
lr_auc = roc_auc_score(LR_test_Labels, logistic_probs)
print('No Skill: ROC AUC=%.3f' % (lR_Prb_auc))
print('Logistic: ROC AUC=%.3f' % (lr_auc))
LR_Prob_fpr, LR_Prob_tpr, _ = roc_curve(LR_test_Labels, LR_Prob)
lr_fpr, lr_tpr, _ = roc_curve(LR_test_Labels, logistic_probs)
pyplot.plot(LR_Prob_fpr, LR_Prob_tpr, linestyle='--', label='No Skill')
pyplot.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
pyplot.legend()
pyplot.show()

LR_model_predict = LRmodel.predict(LR_test_Features)
lr_precision, lr_recall, _ = precision_recall_curve(LR_test_Labels, logistic_probs)
lr_f1, lr_auc = f1_score(LR_test_Labels, LR_model_predict), auc(lr_recall, lr_precision)
print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))
no_skill = len(LR_test_Labels[LR_test_Labels==1]) / len(LR_test_Labels)
pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')
pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')
pyplot.xlabel('Recall')
pyplot.ylabel('Precision')
pyplot.legend()
pyplot.show()
matrix_result = confusion_matrix(y_true=LR_test_Labels, y_pred=LR_model_predict)
fig, ax = graph.subplots(figsize=(5, 5))
ax.matshow(matrix_result, cmap=graph.cm.Oranges, alpha=0.3)
for i in range(matrix_result.shape[0]):
    for j in range(matrix_result.shape[1]):
        ax.text(x=j, y=i,s=matrix_result[i, j], va='center', ha='center', size='xx-large')
 
graph.xlabel('Predictions', fontsize=18)
graph.ylabel('Actuals', fontsize=18)
graph.title('Confusion Matrix', fontsize=18)
graph.show()

evaluateModel(LR_test_Labels,LR_model_predict)

# roc curve and auc
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from matplotlib import pyplot
from sklearn.neighbors import KNeighborsClassifier as knn

# generate 2 class dataset
knnfeatures, knnlabels = make_classification(n_samples=1000, n_classes=2, random_state=1)
# split into train/test sets
knn_features_train, knn_features_test, knn_label_train, knn_label_test = train_test_split(knnfeatures, knnlabels, test_size=0.5, random_state=2)
# generate a no skill prediction (majority class)
ns_probs = [0 for _ in range(len(knn_label_test))]
# fit a model
model=knn(n_neighbors=5)
model.fit(knn_features_train, knn_label_train)
# predict probabilities
KNN_probs = model.predict_proba(knn_features_test)
# keep probabilities for the positive outcome only
KNN_probs = KNN_probs[:, 1]
# calculate scores
KNN_prob_auc = roc_auc_score(knn_label_test, ns_probs)
KNN_auc = roc_auc_score(knn_label_test, KNN_probs)
# summarize scores
print('No Skill: ROC AUC=%.3f' % (KNN_prob_auc))
print('KNN: ROC AUC=%.3f' % (KNN_auc))
# calculate roc curves
KNN_fpr, KNN_tpr, _ = roc_curve(knn_label_test, ns_probs)
KNN_fpr, KNN_tpr, _ = roc_curve(knn_label_test, KNN_probs)
# plot the roc curve for the model
pyplot.plot(KNN_fpr, KNN_tpr, linestyle='--', label='No Skill')
pyplot.plot(KNN_fpr, KNN_tpr, marker='.', label='KNN')
# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
# show the legend
pyplot.legend()
# show the plot
pyplot.show()
KNN_Model_predict = model.predict(knn_features_test)
KNN_precision, KNN_recall, _ = precision_recall_curve(knn_label_test, KNN_probs)
KNN_f1, KNN_auc = f1_score(knn_label_test, KNN_Model_predict), auc(KNN_recall, KNN_precision)
# summarize scores
print('Logistic: f1=%.3f auc=%.3f' % (KNN_f1, KNN_auc))
# plot the precision-recall curves
no_skill = len(knn_label_test[knn_label_test==1]) / len(knn_label_test)
pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')
pyplot.plot(KNN_recall, KNN_precision, marker='.', label='Logistic')
# axis labels
pyplot.xlabel('Recall')
pyplot.ylabel('Precision')
# show the legend
pyplot.legend()
# show the plot
pyplot.show()

KNN_Matrix = confusion_matrix(y_true=knn_label_test, y_pred=KNN_Model_predict)
#
# Print the confusion matrix using Matplotlib
#
fig, ax = graph.subplots(figsize=(5, 5))
ax.matshow(KNN_Matrix, cmap=graph.cm.Oranges, alpha=0.3)
for i in range(KNN_Matrix.shape[0]):
    for j in range(KNN_Matrix.shape[1]):
        ax.text(x=j, y=i,s=KNN_Matrix[i, j], va='center', ha='center', size='xx-large')
 
graph.xlabel('Predictions', fontsize=18)
graph.ylabel('Actuals', fontsize=18)
graph.title('Confusion Matrix', fontsize=18)
graph.show()

evaluateModel(knn_label_test,KNN_Model_predict)

# roc curve and auc
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from matplotlib import pyplot
from sklearn.svm import SVC

# generate 2 class dataset
SVCFeatures, SVCLabels = make_classification(n_samples=1000, n_classes=2, random_state=1)
# split into train/test sets
SVCtrainFeatures, SVCtestfeatures, SVCtrainlabels, SVCtestlabels = train_test_split(SVCFeatures, SVCLabels, test_size=0.5, random_state=2)
# generate a no skill prediction (majority class)
ns_probs = [0 for _ in range(len(SVCtestlabels))]
# fit a model
svcmodel=SVC(kernel='rbf', random_state=0,probability=True)
svcmodel.fit(SVCtrainFeatures, SVCtrainlabels)
# predict probabilities
svc_probs = svcmodel.predict_proba(SVCtestfeatures)
# keep probabilities for the positive outcome only
svc_probs = svc_probs[:, 1]
# calculate scores
svc_prob_acc = roc_auc_score(SVCtestlabels, ns_probs)
svc_auc = roc_auc_score(SVCtestlabels, svc_probs)
# summarize scores
print('No Skill: ROC AUC=%.3f' % (svc_prob_acc))
print('SVC: ROC AUC=%.3f' % (svc_auc))
# calculate roc curves
svc_fpr, svc_tpr, _ = roc_curve(SVCtestlabels, ns_probs)
sv_fpr, sv_tpr, _ = roc_curve(SVCtestlabels, svc_probs)
# plot the roc curve for the model
pyplot.plot(svc_fpr, svc_tpr, linestyle='--', label='No Skill')
pyplot.plot(sv_fpr, sv_tpr, marker='.', label='SVC')
# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
# show the legend
pyplot.legend()
# show the plot
pyplot.show()

svcmodelpredict = svcmodel.predict(SVCtestfeatures)
svc_precision, svc_recall, _ = precision_recall_curve(SVCtestlabels, svc_probs)
svc_f1, svc_auc = f1_score(SVCtestlabels, svcmodelpredict), auc(svc_recall, svc_precision)
# summarize scores
print('SVC: f1=%.3f auc=%.3f' % (svc_f1, svc_auc))
# plot the precision-recall curves
no_skill = len(SVCtestlabels[SVCtestlabels==1]) / len(SVCtestlabels)
pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')
pyplot.plot(svc_recall, svc_precision, marker='.', label='SVC')
# axis labels
pyplot.xlabel('Recall')
pyplot.ylabel('Precision')
# show the legend
pyplot.legend()
# show the plot
pyplot.show()

svc_con_matrix = confusion_matrix(y_true=SVCtestlabels, y_pred=svcmodelpredict)
#
# Print the confusion matrix using Matplotlib
#
fig, ax = graph.subplots(figsize=(5, 5))
ax.matshow(svc_con_matrix, cmap=graph.cm.Oranges, alpha=0.3)
for i in range(svc_con_matrix.shape[0]):
    for j in range(svc_con_matrix.shape[1]):
        ax.text(x=j, y=i,s=svc_con_matrix[i, j], va='center', ha='center', size='xx-large')
 
graph.xlabel('Predictions', fontsize=18)
graph.ylabel('Actuals', fontsize=18)
graph.title('Confusion Matrix', fontsize=18)
graph.show()

evaluateModel(SVCtestlabels,svcmodelpredict)

# roc curve and auc
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from matplotlib import pyplot
from sklearn.naive_bayes import GaussianNB as GNB


# generate 2 class dataset
GNBFeatures, GNBLabels = make_classification(n_samples=1000, n_classes=2, random_state=1)
# split into train/test sets
GNBtrainfeatures, GNBtestfeatures, GNBtrainlabels,GNGtestlabels = train_test_split(GNBFeatures, GNBLabels, test_size=0.5, random_state=2)
# generate a no skill prediction (majority class)
ns_probs = [0 for _ in range(len(GNGtestlabels))]
# fit a model
Nbmodel=GNB()
Nbmodel.fit(GNBtrainfeatures, GNBtrainlabels)
# predict probabilities
GNB_probs = Nbmodel.predict_proba(GNBtestfeatures)
# keep probabilities for the positive outcome only
GNB_probs = GNB_probs[:, 1]
# calculate scores
svc_auc = roc_auc_score(GNGtestlabels, ns_probs)
sv_auc = roc_auc_score(GNGtestlabels, GNB_probs)
# summarize scores
print('No Skill: ROC AUC=%.3f' % (svc_auc))
print('SVC: ROC AUC=%.3f' % (sv_auc))
# calculate roc curves
svc_fpr, svc_tpr, _ = roc_curve(GNGtestlabels, ns_probs)
sv_fpr, sv_tpr, _ = roc_curve(GNGtestlabels, GNB_probs)
# plot the roc curve for the model
pyplot.plot(svc_fpr, svc_tpr, linestyle='--', label='No Skill')
pyplot.plot(sv_fpr, sv_tpr, marker='.', label='SVC')
# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
# show the legend
pyplot.legend()
# show the plot
pyplot.show()

gnbmodelpredict = Nbmodel.predict(GNBtestfeatures)
svc_precision,svc_recall, _ = precision_recall_curve(GNGtestlabels, GNB_probs)
svc_f1, sv_auc = f1_score(GNGtestlabels, gnbmodelpredict), auc(svc_recall, svc_precision)
# summarize scores
print('GNB: f1=%.3f auc=%.3f' % (svc_f1, sv_auc))
# plot the precision-recall curves
no_skill = len(GNGtestlabels[GNGtestlabels==1]) / len(GNGtestlabels)
pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')
pyplot.plot(svc_recall, svc_precision, marker='.', label='GNB')
# axis labels
pyplot.xlabel('Recall')
pyplot.ylabel('Precision')
# show the legend
pyplot.legend()
# show the plot
pyplot.show()

GNB_Con_matrix = confusion_matrix(y_true=GNGtestlabels, y_pred=gnbmodelpredict)
#
# Print the confusion matrix using Matplotlib
#
fig, ax = graph.subplots(figsize=(5, 5))
ax.matshow(GNB_Con_matrix, cmap=graph.cm.Oranges, alpha=0.3)
for i in range(GNB_Con_matrix.shape[0]):
    for j in range(GNB_Con_matrix.shape[1]):
        ax.text(x=j, y=i,s=GNB_Con_matrix[i, j], va='center', ha='center', size='xx-large')
 
graph.xlabel('Predictions', fontsize=18)
graph.ylabel('Actuals', fontsize=18)
graph.title('Confusion Matrix', fontsize=18)
graph.show()

evaluateModel(GNGtestlabels,gnbmodelpredict)

# roc curve and auc
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from matplotlib import pyplot
from sklearn.ensemble import RandomForestClassifier as rf


# generate 2 class dataset
RFCFeatures, RFCLabels = make_classification(n_samples=1000, n_classes=2, random_state=1)
# split into train/test sets
RFCtrainfeatures, RFCtestfeatures, RFCtrainlabels, RFCtestlabels = train_test_split(RFCFeatures, RFCLabels, test_size=0.5, random_state=2)
# generate a no skill prediction (majority class)
ns_probs = [0 for _ in range(len(RFCtestlabels))]
# fit a model
rfcmodel= rf(n_estimators=10,criterion='entropy',random_state=0)
rfcmodel.fit(RFCtrainfeatures, RFCtrainlabels)
# predict probabilities
rfc_probs = rfcmodel.predict_proba(RFCtestfeatures)
# keep probabilities for the positive outcome only
rfc_probs = rfc_probs[:, 1]
# calculate scores
nrfc_auc = roc_auc_score(RFCtestlabels, ns_probs)
rfc_auc = roc_auc_score(RFCtestlabels, rfc_probs)
# summarize scores
print('No Skill: ROC AUC=%.3f' % (nrfc_auc))
print('RFC: ROC AUC=%.3f' % (rfc_auc))
# calculate roc curves
nrfc_fpr, nrfc_tpr, _ = roc_curve(RFCtestlabels, ns_probs)
rfc_fpr, rfc_tpr, _ = roc_curve(RFCtestlabels, rfc_probs)
# plot the roc curve for the model
pyplot.plot(nrfc_fpr, nrfc_tpr, linestyle='--', label='No Skill')
pyplot.plot(rfc_fpr, rfc_tpr, marker='.', label='RFC')
# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
# show the legend
pyplot.legend()
# show the plot
pyplot.show()

RCCModelpredict = rfcmodel.predict(RFCtestfeatures)
RFC_precision, RFC_recall, _ = precision_recall_curve(RFCtestlabels, rfc_probs)
lr_f1, rfc_auc = f1_score(RFCtestlabels, RCCModelpredict), auc(RFC_recall, RFC_precision)
# summarize scores
print('RFC: f1=%.3f auc=%.3f' % (lr_f1, rfc_auc))
# plot the precision-recall curves
no_skill = len(RFCtestlabels[RFCtestlabels==1]) / len(RFCtestlabels)
pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')
pyplot.plot(RFC_recall, RFC_precision, marker='.', label='Logistic')
# axis labels
pyplot.xlabel('Recall')
pyplot.ylabel('Precision')
# show the legend
pyplot.legend()
# show the plot
pyplot.show()

 
RFC_matrix = confusion_matrix(y_true=RFCtestlabels, y_pred=RCCModelpredict)
#
# Print the confusion matrix using Matplotlib
#
fig, ax = graph.subplots(figsize=(5, 5))
ax.matshow(RFC_matrix, cmap=graph.cm.Oranges, alpha=0.3)
for i in range(RFC_matrix.shape[0]):
    for j in range(RFC_matrix.shape[1]):
        ax.text(x=j, y=i,s=RFC_matrix[i, j], va='center', ha='center', size='xx-large')
 
graph.xlabel('Predictions', fontsize=18)
graph.ylabel('Actuals', fontsize=18)
graph.title('Confusion Matrix', fontsize=18)
graph.show()

evaluateModel(RFCtestlabels,RCCModelpredict)

# Proposed Algorithm

class bot_url_detection(object):
    def _init_(self):
        pass

    def splitdata(df):
        msk = num.random.rand(len(df)) < 0.75
        train, test = df[msk], df[~msk]
        X_train, y_train = train, train.iloc[:,-1]
        X_test, y_test = test, test.iloc[:, -1]
        return (X_train, y_train, X_test, y_test)

    def predict_result(data):
        Algo_training_data = data.copy()
        # converting id to int
        Algo_training_data['id'] = Algo_training_data.id.apply(lambda x: int(x))
        bag_of_words_bot = r'BOT|Bot|bot|b0t|cannabis|tweet me|mishear|follow me|updates every|gorilla|yes_ofc|forget' \
                           r'expos|kill|clit|bbb|butt|fuck|XXX|sex|truthe|fake|anony|free|virus|funky|RNA|kuck|jargon' \
                           r'nerd|swag|jack|bang|bonsai|chick|prison|paper|pokem|xx|freak|ffd|dunia|clone|genie|bbb' \
                           r'ffd|onlyman|emoji|joke|troll|droop|free|every|wow|cheese|yeah|bio|magic|wizard|face'


        # converting verified into vectors
        Algo_training_data['verified'] = Algo_training_data.verified.apply(lambda x: 1 if ((x == True) or x == 'TRUE') else 0)
        # check if the name contains bot or screenname contains b0t
        verifieddata = ((Algo_training_data.name.str.contains(bag_of_words_bot, case=False, na=False)) |
                     (Algo_training_data.description.str.contains(bag_of_words_bot, case=False, na=False)) |
                     (Algo_training_data.screen_name.str.contains(bag_of_words_bot, case=False, na=False)) |
                     (Algo_training_data.status.str.contains(bag_of_words_bot, case=False, na=False))
                     )  # these all are bots
        Algo_pred = Algo_training_data[verifieddata]  # these all are bots
        Algo_pred.bot = 1
        Algo_pred = Algo_pred[['id', 'bot']]

        # check if the user is verified
        userverify = Algo_training_data[~verifieddata]
        verifieddata = (userverify.verified == 1)  # these all are nonbots
        Algo_pred1 = userverify[verifieddata][['id', 'bot']]
        Algo_pred1.bot = 0
        Algo_pred = pd.concat([Algo_pred, Algo_pred1])

        # check if description contains buzzfeed
        verfiybuzz = userverify[~verifieddata]
        verifieddata = (verfiybuzz.description.str.contains("buzzfeed", case=False, na=False))  # these all are nonbots
        Algo_pred1 = verfiybuzz[verfiybuzz.description.str.contains("buzzfeed", case=False, na=False)][['id', 'bot']]
        Algo_pred1.bot = 0
        Algo_pred = pd.concat([Algo_pred, Algo_pred1])

        # check if listed_count>16000
        countverify = verfiybuzz[~verifieddata]
        countverify.listed_count = countverify.listed_count.apply(lambda x: 0 if x == 'None' else x)
        countverify.listed_count = countverify.listed_count.apply(lambda x: int(x))
        verifieddata = (countverify.listed_count > 16000)  # these all are nonbots
        Algo_pred1 = countverify[verifieddata][['id', 'bot']]
        Algo_pred1.bot = 0
        Algo_pred = pd.concat([Algo_pred, Algo_pred1])
        #remaining
        Algo_pred1 = countverify[~verifieddata][['id', 'bot']]
        Algo_pred1.bot = 0 # these all are nonbots
        Algo_pred = pd.concat([Algo_pred, Algo_pred1])
        return Algo_pred

    def get_predicted_and_true_values(trainfeature, trainlabel):
        y_pred, y_true = bot_url_detection.predict_result(trainfeature).bot.tolist(), trainlabel.tolist()
        return (y_pred, y_true)

    def calculate_accuracy(traindata):
        (trainfeatures, trainlabels, testfeatures, testlabels) = bot_url_detection.splitdata(traindata)
        # predictions on training data
        predicted_trainlabel, predicted_true_label = bot_url_detection.get_predicted_and_true_values(trainfeatures, trainlabels)
        evaluateModel(predicted_trainlabel, predicted_true_label)

if _name_ == '_main_':
    Algo_trainfeatureslabels = pd.read_csv('Botdataset.csv')
    Algo_testfeatureslabels = pd.read_csv('Botdata.csv')
    

    Algo_trainfeatureslabels.drop(['id_str', 'location', 'url', 'created_at', 'lang', 'has_extended_profile'],axis=1,inplace=True)
    Algo_testfeatureslabels.drop(['id_str', 'location', 'url', 'created_at', 'lang', 'has_extended_profile'],axis=1,inplace=True)  
    Algo_pred = bot_url_detection.predict_result(Algo_testfeatureslabels)   
    bot_url_detection.calculate_accuracy(Algo_trainfeatureslabels)

# roc curve and auc
from sklearn.datasets import make_classification
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from matplotlib import pyplot
from sklearn.naive_bayes import GaussianNB as GNB


# generate 2 class dataset
X, y = make_classification(n_samples=1000, n_classes=2, random_state=1)
# split into train/test sets
trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.5, random_state=2)
# generate a no skill prediction (majority class)
ns_probs = [0 for _ in range(len(testy))]
# fit a model
# Instantiate Gradient Boosting Regressor
gbc = GradientBoostingClassifier(n_estimators=300,
                                 learning_rate=0.05,
                                 random_state=100,
                                 max_features=5 )
gbc.fit(trainX, trainy)
# predict probabilities
lr_probs = gbc.predict_proba(testX)
# keep probabilities for the positive outcome only
lr_probs = lr_probs[:, 1]
# calculate scores
ns_auc = roc_auc_score(testy, ns_probs)
lr_auc = roc_auc_score(testy, lr_probs)
# summarize scores
print('No Skill: ROC AUC=%.3f' % (ns_auc))
print('SVC: ROC AUC=%.3f' % (lr_auc))
# calculate roc curves
ns_fpr, ns_tpr, _ = roc_curve(testy, ns_probs)
lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs)
# plot the roc curve for the model
pyplot.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')
pyplot.plot(lr_fpr, lr_tpr, marker='.', label='SVC')
# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
# show the legend
pyplot.legend()
# show the plot
pyplot.show()

yhat = gbc.predict(testX)
lr_precision, lr_recall, _ = precision_recall_curve(testy, lr_probs)
lr_f1, lr_auc = f1_score(testy, yhat), auc(lr_recall, lr_precision)
# summarize scores
print('Logistic: f1=%.3f auc=%.3f' % (lr_f1, lr_auc))
# plot the precision-recall curves
no_skill = len(testy[testy==1]) / len(testy)
pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')
pyplot.plot(lr_recall, lr_precision, marker='.', label='Logistic')
# axis labels
pyplot.xlabel('Recall')
pyplot.ylabel('Precision')
# show the legend
pyplot.legend()
# show the plot
pyplot.show()

conf_matrix = confusion_matrix(y_true=testy, y_pred=yhat)
#
# Print the confusion matrix using Matplotlib
#
fig, ax = graph.subplots(figsize=(5, 5))
ax.matshow(conf_matrix, cmap=graph.cm.Oranges, alpha=0.3)
for i in range(conf_matrix.shape[0]):
    for j in range(conf_matrix.shape[1]):
        ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='xx-large')
 
graph.xlabel('Predictions', fontsize=18)
graph.ylabel('Actuals', fontsize=18)
graph.title('Confusion Matrix', fontsize=18)
graph.show()

evaluateModel(testy,yhat)

RFC=82.2000
GNB=80.8000
SVC=84.2000
KNN=79.6000
LR=83.4000
AL=97.2590
GB=82.800


import matplotlib.pyplot as plt
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
x=['RandomForest','SVM','LR','NB','KNN','AL','GB']
y = [RFC, SVC, LR,GNB, KNN,AL,GB]
ax.bar(x,y)
plt.show()

from sklearn.ensemble import VotingClassifier
from sklearn.metrics import classification_report, confusion_matrix

from sklearn.ensemble import BaggingClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import AdaBoostClassifier
 
#create a dictionary of base learners
estimators=[('knn',model),('nb', Nbmodel)]
#create voting classifier
clf = AdaBoostClassifier(n_estimators=100) 
#fit model to training data
clf.fit(RFCtrainfeatures, RFCtrainlabels) 
# get predictions from best model above
y_preds_mv = clf.predict(RFCtestfeatures)
print('Adaboost: ',clf.score(RFCtestfeatures, RFCtestlabels)*100)
print('\n')
 

# roc curve and auc
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from matplotlib import pyplot
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import classification_report, confusion_matrix
import joblib

# generate 2 class dataset
mvfeatures, mvlabels = make_classification(n_samples=1000, n_classes=2, random_state=1)
# split into train/test sets
mvtrainfeatures, mvtestfeatures, mvtrainlabels, mvtestlabels = train_test_split(mvfeatures, mvlabels, test_size=0.5, random_state=2)
# generate a no skill prediction (majority class)
ns_probs = [0 for _ in range(len(mvtestlabels))]
# fit a model
estimators=[('rfc', rfcmodel), ('knn',model),('nb', Nbmodel)]
majority_voting = VotingClassifier(estimators, voting='soft')
majority_voting.fit(mvtrainfeatures, mvtrainlabels)
# predict probabilities
mv_probs = majority_voting.predict_proba(mvtestfeatures)
# keep probabilities for the positive outcome only
mv_probs = mv_probs[:, 1]
# calculate scores
nmv_auc = roc_auc_score(mvtestlabels, ns_probs)
mv_auc = roc_auc_score(mvtestlabels, mv_probs)
# summarize scores
print('No Skill: ROC AUC=%.3f' % (nmv_auc))
print('MV: ROC AUC=%.3f' % (mv_auc))
# calculate roc curves
nmv_fpr, nmv_tpr, _ = roc_curve(mvtestlabels, ns_probs)
mv_fpr, mv_tpr, _ = roc_curve(mvtestlabels, mv_probs)
# plot the roc curve for the model
pyplot.plot(nmv_fpr, nmv_tpr, linestyle='--', label='No Skill')
pyplot.plot(mv_fpr, mv_tpr, marker='.', label='MV')
# axis labels
pyplot.xlabel('False Positive Rate')
pyplot.ylabel('True Positive Rate')
# show the legend
pyplot.legend()
# show the plot
pyplot.show()

mvmodelpredict = majority_voting.predict(mvtestfeatures)
mv_precision, mv_recall, _ = precision_recall_curve(mvtestlabels, mv_probs)
mv_f1, mv_auc = f1_score(mvtestlabels, mvmodelpredict), auc(mv_recall, mv_precision)
# summarize scores
print('MV: f1=%.3f auc=%.3f' % (mv_f1, mv_auc))
# plot the precision-recall curves
no_skill = len(mvtestlabels[mvtestlabels==1]) / len(mvtestlabels)
pyplot.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')
pyplot.plot(mv_recall, mv_precision, marker='.', label='MV')
# axis labels
pyplot.xlabel('Recall')
pyplot.ylabel('Precision')
# show the legend
pyplot.legend()
# show the plot
pyplot.show()

 
mvmatrix = confusion_matrix(y_true=mvtestlabels, y_pred=mvmodelpredict)
#
# Print the confusion matrix using Matplotlib
#
fig, ax = graph.subplots(figsize=(5, 5))
ax.matshow(mvmatrix, cmap=graph.cm.Oranges, alpha=0.3)
for i in range(mvmatrix.shape[0]):
    for j in range(mvmatrix.shape[1]):
        ax.text(x=j, y=i,s=mvmatrix[i, j], va='center', ha='center', size='xx-large')
 
graph.xlabel('Predictions', fontsize=18)
graph.ylabel('Actuals', fontsize=18)
graph.title('Confusion Matrix', fontsize=18)
graph.show()

evaluateModel(mvtestlabels,mvmodelpredict)
